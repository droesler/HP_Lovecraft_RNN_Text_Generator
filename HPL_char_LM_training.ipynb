{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HPL_char_LM_generator_Clean2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae34oNJH-JmZ"
      },
      "source": [
        "# H.P. Lovecraft Character-level RNN Text Generator\n",
        "This is the notebook that was used to train the TensorFlow generator model. The code is based on the TensorFlow RNN tutorial that can be found at https://www.tensorflow.org/text/tutorials/text_generation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJCd9mg3sWXa"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnKFTErBn-42"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftvZpSresbFd"
      },
      "source": [
        "# Download corpus (2.75 MB) from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQTh0Y2TsFE_",
        "outputId": "12d92750-c4b5-4237-96ae-b30ada8f9064"
      },
      "source": [
        "!wget https://github.com/droesler/HP_Lovecraft_RNN_Text_Generator/raw/main/lovecraft_split_sentences.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-17 21:36:49--  https://github.com/droesler/HP_Lovecraft_RNN_Text_Generator/raw/main/lovecraft_split_sentences.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/droesler/HP_Lovecraft_RNN_Text_Generator/main/lovecraft_split_sentences.txt [following]\n",
            "--2021-09-17 21:36:49--  https://raw.githubusercontent.com/droesler/HP_Lovecraft_RNN_Text_Generator/main/lovecraft_split_sentences.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2886674 (2.8M) [text/plain]\n",
            "Saving to: ‘lovecraft_split_sentences.txt’\n",
            "\n",
            "lovecraft_split_sen 100%[===================>]   2.75M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-09-17 21:36:50 (45.7 MB/s) - ‘lovecraft_split_sentences.txt’ saved [2886674/2886674]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjdvnU2Nsqi9"
      },
      "source": [
        "# Prepare Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg4WAQlJp4xm",
        "outputId": "e2403f1a-9fbd-47ae-86ce-f3eb47c5554b"
      },
      "source": [
        "# Read corpus file\n",
        "path_to_file = './lovecraft_split_sentences.txt'\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 2886590 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1bDLC2NqnY1",
        "outputId": "266c53cc-bd0a-46b1-a1ec-bf7b26dbf3c3"
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE NAMELESS CITY\n",
            "\n",
            "When I drew nigh the nameless city I knew it was accursed.\n",
            "I was traveling in a parched and terrible valley under the moon, and afar I saw it protruding uncannily above the sands as parts of a corpse may protrude from an ill-made g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S99kMmlXqn3X",
        "outputId": "74378e8c-cc18-4577-d598-c9df860a2eee"
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHOM-QwBrGU4"
      },
      "source": [
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20yVMiQkrTX3"
      },
      "source": [
        "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diL51xalr0en"
      },
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15f8tj_EsRwn",
        "outputId": "af180f65-361e-40a3-ca3d-d6c714570738"
      },
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2886590,), dtype=int64, numpy=array([47, 35, 32, ..., 46, 12,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ec49c74tJhX"
      },
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heGWd4c4tzuM"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw-i0Au0t0Rn",
        "outputId": "b9a60e03-3199-4e08-b0b2-a830633152ae"
      },
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'T' b'H' b'E' b' ' b'N' b'A' b'M' b'E' b'L' b'E' b'S' b'S' b' ' b'C'\n",
            " b'I' b'T' b'Y' b'\\n' b'\\n' b'W' b'h' b'e' b'n' b' ' b'I' b' ' b'd' b'r'\n",
            " b'e' b'w' b' ' b'n' b'i' b'g' b'h' b' ' b't' b'h' b'e' b' ' b'n' b'a'\n",
            " b'm' b'e' b'l' b'e' b's' b's' b' ' b'c' b'i' b't' b'y' b' ' b'I' b' '\n",
            " b'k' b'n' b'e' b'w' b' ' b'i' b't' b' ' b'w' b'a' b's' b' ' b'a' b'c'\n",
            " b'c' b'u' b'r' b's' b'e' b'd' b'.' b'\\n' b'I' b' ' b'w' b'a' b's' b' '\n",
            " b't' b'r' b'a' b'v' b'e' b'l' b'i' b'n' b'g' b' ' b'i' b'n' b' ' b'a'\n",
            " b' ' b'p' b'a'], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkxgQHw1uGVm",
        "outputId": "8c896a1b-2a27-476d-e5f6-a87e3688bd18"
      },
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'THE NAMELESS CITY\\n\\nWhen I drew nigh the nameless city I knew it was accursed.\\nI was traveling in a pa'\n",
            "b'rched and terrible valley under the moon, and afar I saw it protruding uncannily above the sands as p'\n",
            "b'arts of a corpse may protrude from an ill-made grave.\\nFear spoke from the age-worn stones of this hoa'\n",
            "b'ry survivor of the deluge, this great-grandfather of the eldest pyramid; and a viewless aura repelled'\n",
            "b' me and bade me retreat from antique and sinister secrets that no man should see, and no man else had'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wpyn3vNnumUO"
      },
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7x7s6h6upEm"
      },
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip22G1Q_u52v",
        "outputId": "1b35c237-5afd-4e62-aa09-be3798409a0f"
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'THE NAMELESS CITY\\n\\nWhen I drew nigh the nameless city I knew it was accursed.\\nI was traveling in a p'\n",
            "Target: b'HE NAMELESS CITY\\n\\nWhen I drew nigh the nameless city I knew it was accursed.\\nI was traveling in a pa'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgjwykjJu6SO",
        "outputId": "5de5e2a3-ac1b-43ec-a077-204c8196ff6a"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwwHAUvuvhn5"
      },
      "source": [
        "# Create the language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdfbQCBcvAbm"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrQ_rGKRvUFV"
      },
      "source": [
        "# If/else conditions are used so that the model is able to predict based on \n",
        "# sequences, or read in a previous pair of hidden states in a single step mode.\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru_1 = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.gru_2 = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states_list=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    if states_list is None:\n",
        "      x, state_1 = self.gru_1(x, training=training)\n",
        "      x, state_2 = self.gru_2(x, training=training)\n",
        "      \n",
        "    else:\n",
        "      x, state_1 = self.gru_1(x, initial_state=states_list[0], training=training)\n",
        "      x, state_2 = self.gru_2(x, initial_state=states_list[1], training=training)\n",
        "    \n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      states_list = []\n",
        "      states_list.extend([state_1, state_2])\n",
        "      return x, states_list\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbsmNgtkvsYu"
      },
      "source": [
        "model = MyModel(\n",
        "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veXAj__Pv6PR",
        "outputId": "1d7aeaee-9a12-4bc3-f010-1459cf2d9ad0"
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 92) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJMITpk-wSZh",
        "outputId": "2fe2f0ba-3dba-428f-9eb2-87e2f94e01eb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      multiple                  23552     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  multiple                  3938304   \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  multiple                  6297600   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  94300     \n",
            "=================================================================\n",
            "Total params: 10,353,756\n",
            "Trainable params: 10,353,756\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0c4lL8EwS40"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYPD9coaw6hg",
        "outputId": "7cfea4a3-ad67-4878-ed93-40143250a06d"
      },
      "source": [
        "# Show the loss before training\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", mean_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 92)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         4.5221515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33Le1lxSxJL0"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTVSYAVMv8df"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHjeAQ0xY3S"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UBDPaZwyppv"
      },
      "source": [
        "EPOCHS = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1aKLySYy3-T",
        "outputId": "bd5d7222-3c76-4c50-d0b6-4c74c0c2c29f"
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "446/446 [==============================] - 57s 120ms/step - loss: 2.0570\n",
            "Epoch 2/30\n",
            "446/446 [==============================] - 57s 124ms/step - loss: 1.4273\n",
            "Epoch 3/30\n",
            "446/446 [==============================] - 58s 127ms/step - loss: 1.2954\n",
            "Epoch 4/30\n",
            "446/446 [==============================] - 59s 129ms/step - loss: 1.2311\n",
            "Epoch 5/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 1.1850\n",
            "Epoch 6/30\n",
            "446/446 [==============================] - 59s 131ms/step - loss: 1.1442\n",
            "Epoch 7/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 1.1047\n",
            "Epoch 8/30\n",
            "446/446 [==============================] - 60s 131ms/step - loss: 1.0652\n",
            "Epoch 9/30\n",
            "446/446 [==============================] - 59s 129ms/step - loss: 1.0247\n",
            "Epoch 10/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.9843\n",
            "Epoch 11/30\n",
            "446/446 [==============================] - 60s 131ms/step - loss: 0.9430\n",
            "Epoch 12/30\n",
            "446/446 [==============================] - 59s 129ms/step - loss: 0.9033\n",
            "Epoch 13/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.8650\n",
            "Epoch 14/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.8296\n",
            "Epoch 15/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.7973\n",
            "Epoch 16/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.7681\n",
            "Epoch 17/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.7418\n",
            "Epoch 18/30\n",
            "446/446 [==============================] - 59s 130ms/step - loss: 0.7183\n",
            "Epoch 19/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6992\n",
            "Epoch 20/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6821\n",
            "Epoch 21/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6684\n",
            "Epoch 22/30\n",
            "446/446 [==============================] - 59s 130ms/step - loss: 0.6544\n",
            "Epoch 23/30\n",
            "446/446 [==============================] - 58s 129ms/step - loss: 0.6445\n",
            "Epoch 24/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6362\n",
            "Epoch 25/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6280\n",
            "Epoch 26/30\n",
            "446/446 [==============================] - 58s 127ms/step - loss: 0.6215\n",
            "Epoch 27/30\n",
            "446/446 [==============================] - 59s 130ms/step - loss: 0.6161\n",
            "Epoch 28/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6147\n",
            "Epoch 29/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6135\n",
            "Epoch 30/30\n",
            "446/446 [==============================] - 58s 128ms/step - loss: 0.6100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmHWpnwJwJqz"
      },
      "source": [
        "# Create generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3b8EH3MzBRj"
      },
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.25):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states_list=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states_list = self.model(inputs=input_ids, states_list=states_list,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkGGBoFmzE42"
      },
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzvyh6MhwNXW"
      },
      "source": [
        "# Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EFxauLnzKWG",
        "outputId": "f1111cba-f31d-405a-f0e3-c12a8b816569"
      },
      "source": [
        "start = time.time()\n",
        "states_list = None\n",
        "initial_prompt = 'The blasphemous'\n",
        "next_char = tf.constant([initial_prompt])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states_list = one_step_model.generate_one_step(next_char, states_list=states_list)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The blasphemous place he had written me a very perceptible resemblance--or it was a paint and almost hidden and unaccountably lower to travel alone.\n",
            "When I did glance down the street at a time without part that a mere mistake had been gruesomely dangerous that Kalos showed them in the air of the missing shed no light, and the curious inhibitions suggested by his companions and came again, and for all the human race had been the recorded speech concerned with the planetary glass of a human skull at a point clutched at its memory and more frequent is carved in London-window.\n",
            "At last he had seen what we read of proportion in the antarctic, while at one side I could not well uncompany her.\n",
            "I handed the door and all the time for position at all, but which had been a mistake; for they were not because they were so running with the supply of the guidance or purclasion.\n",
            "She was not much more than a mask for almost unassistable fright--for in all that world of day the land dwelt in the black woods, he was sin \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.110875129699707\n"
          ]
        }
      ]
    }
  ]
}